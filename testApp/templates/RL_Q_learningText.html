{% extends 'base.html' %}
{% block body %}
{% load static %}
<title>Q learning-BanglayML</title>

<div class="row">
        <div class="col s10 push-s1">
            <div class="card">
                <div class="card-content">
                    <span class="card-title center-align deep-orange-text text-darken-2">Q-Learning algorithm</span>
                    <ul class="collapsible expandable">
                        <li class="active">
                            <div class="collapsible-header blue lighten-5" style="font-size: 20px;">Blog</div>
                                <div class="collapsible-body">
                                    <p class="center-left">
                                        ‡¶Ü‡¶ú ‡¶Ü‡¶Æ‡¶∞‡¶æ Reinforcement Learning- ‡¶è‡¶∞ Q-learning- ‡¶∏‡¶Æ‡ßç‡¶™‡¶∞‡ßç‡¶ï‡ßá ‡¶ú‡¶æ‡¶®‡¶¨‡•§
                                        ‡¶ï‡ßã‡¶® ‡¶è‡¶ï‡¶ü‡¶æ ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡¶ø‡¶∑‡ßç‡¶ü ‡¶∏‡ßç‡¶ü‡ßá‡¶ü‡ßá ‡¶ï‡ßã‡¶® action/decision ‡¶®‡¶ø‡¶≤‡ßá ‡¶ü‡ßã‡¶ü‡¶æ‡¶≤ reward ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ï‡ßç‡¶∏‡¶ø‡¶Æ‡¶æ‡¶Æ ‡¶π‡¶¨‡ßá, ‡¶∏‡ßá‡¶ü‡¶æ Q-learning ‡¶è‡¶∞ ‡¶∏‡¶æ‡¶π‡¶æ‡¶Ø‡ßç‡¶Ø‡ßá ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡•§
                                        ‡¶è‡¶∞ ‡¶∏‡¶æ‡¶π‡¶æ‡¶Ø‡ßç‡¶Ø‡ßá state-action pair-‡¶ï‡ßá ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶™ ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡¶§‡ßá Gained reward ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ï‡ßç‡¶∏‡¶ø‡¶Æ‡¶æ‡¶Æ ‡¶π‡ßü‡•§<br><br>
                                        Q-learning ‡¶∏‡¶Æ‡ßç‡¶™‡¶∞‡ßç‡¶ï‡ßá ‡¶ú‡¶æ‡¶®‡¶æ‡¶∞ ‡¶Ü‡¶ó‡ßá ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶õ‡ßã‡¶ü ‡¶õ‡ßã‡¶ü ‡¶¨‡¶ø‡¶∑‡ßü ‡¶∏‡¶Æ‡ßç‡¶™‡¶∞‡ßç‡¶ï‡ßá ‡¶ú‡ßá‡¶®‡ßá ‡¶®‡¶ø‡¶¨‡•§<br>
                                        <span class="deep-orange-text text-darken-2">Policy:</span> Policy ‡¶Æ‡ßÅ‡¶≤‡¶§ ‡¶ï‡ßã‡¶® ‡¶è‡¶ï‡¶ü‡¶æ ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡¶ø‡¶∑‡ßç‡¶ü ‡¶∏‡ßç‡¶ü‡ßá‡¶ü‡ßá ‡¶•‡¶æ‡¶ï‡¶æ ‡¶Ö‡¶¨‡¶∏‡ßç‡¶•‡¶æ‡ßü ‡¶Ö‡¶á ‡¶∏‡ßç‡¶ü‡ßá‡¶ü ‡¶•‡ßá‡¶ï‡ßá ‡¶∏‡¶Æ‡ßç‡¶≠‡¶æ‡¶¨‡ßç‡¶Ø ‡¶∏‡¶ï‡¶≤ ‡¶™‡ßç‡¶∞‡¶ï‡¶æ‡¶∞ action-‡¶è‡¶∞ probability-‡¶ï‡ßá ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶™ ‡¶ï‡¶∞‡ßá‡•§ 
                                        ‡¶è‡¶ï‡ßá œÄ ‡¶¶‡¶ø‡ßü‡ßá ‡¶™‡ßç‡¶∞‡¶ï‡¶æ‡¶∂ ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡•§<br>
                                        $$ ùúã(a|s) = P(A_t = a | S_t = s) $$
                                        <span class="deep-orange-text text-darken-2">State-value function:</span> ‡¶ï‡ßã‡¶® ‡¶∏‡ßç‡¶ü‡ßá‡¶ü agent-‡¶è‡¶∞ ‡¶ï‡¶§‡¶ü‡¶æ ‡¶≠‡¶æ‡¶≤‡ßã ‡¶¨‡¶æ ‡¶ñ‡¶æ‡¶∞‡¶æ‡¶™ ‡¶è‡¶ü‡¶æ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü ‡¶∏‡ßç‡¶ü‡ßá‡¶ü ‡¶≠‡ßç‡¶Ø‡¶æ‡¶≤‡ßÅ ‡¶´‡¶æ‡¶Ç‡¶∂‡¶® ‡¶¶‡¶ø‡ßü‡ßá‡•§ 
                                        Policy, State-value function- ‡¶è‡¶∞ ‡¶â‡¶™‡¶∞ ‡¶®‡¶ø‡¶∞‡ßç‡¶≠‡¶∞ ‡¶ï‡¶∞‡ßá‡•§
                                        ‡¶§‡¶æ‡¶á ‡¶è‡¶ï‡ßá v_œÄ ‡¶è‡¶∞ ‡¶¶‡ßç‡¶¨‡¶æ‡¶∞‡¶æ ‡¶™‡ßç‡¶∞‡¶ï‡¶æ‡¶∂ ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡•§<br>
                                        $$ v_ùúã(s,a) = E\left(G_t | S_t=s \right) = E\left(\sum_{k=0}^‚àû Œ≥^kR_{t+Œ≥+1} | S_t=s \right) $$
                                        ‡¶è‡¶ñ‡¶æ‡¶®‡ßá Œ≥ ‡¶π‡¶≤ Discount factor. ‡¶è‡¶∞ ‡¶≠‡ßç‡¶Ø‡¶æ‡¶≤‡ßÅ ‡ß¶ ‡¶•‡ßá‡¶ï‡ßá ‡ßß ‡¶è‡¶∞ ‡¶≠‡ßá‡¶§‡¶∞ ‡¶π‡ßü‡ßá ‡¶•‡¶æ‡¶ï‡ßá‡•§
                                        Gained Reward ‡¶Ø‡ßá‡¶® future state-‡¶è‡¶∞ reward ‡¶¶‡ßç‡¶¨‡¶æ‡¶∞‡¶æ ‡¶ï‡¶Æ ‡¶™‡ßç‡¶∞‡¶≠‡¶æ‡¶¨‡¶ø‡¶§ ‡¶π‡ßü ‡¶è‡¶ú‡¶®‡ßç‡¶Ø ‡¶è‡¶ü‡¶æ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡•§ k ‡¶π‡¶≤ ‡¶∏‡ßç‡¶ü‡ßá‡¶ü ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ‡•§<br><br>
                                        <span class="deep-orange-text text-darken-2">Action-value function:</span> ‡¶ï‡ßã‡¶® ‡¶∏‡ßç‡¶ü‡ßá‡¶ü‡ßá ‡¶•‡¶æ‡¶ï‡¶æ ‡¶Ö‡¶¨‡¶∏‡ßç‡¶•‡¶æ‡ßü agent-‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶ï‡ßã‡¶® action-‡¶ü‡¶æ ‡¶≠‡¶æ‡¶≤‡ßã ‡¶¨‡¶æ ‡¶ñ‡¶æ‡¶∞‡¶æ‡¶™ ‡¶∏‡ßá‡¶ü‡¶æ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü Action-value function-‡¶è‡¶∞ ‡¶¶‡ßç‡¶¨‡¶æ‡¶∞‡¶æ‡•§ ‡¶è‡¶ï‡ßá Q-function‡¶ì ‡¶¨‡¶≤‡¶æ ‡¶π‡ßü‡•§
                                        Policy, Q-function ‡¶è‡¶∞ ‡¶â‡¶™‡¶∞‡ßá‡¶ì ‡¶®‡¶ø‡¶∞‡ßç‡¶≠‡¶∞ ‡¶ï‡¶∞‡ßá‡•§ ‡¶§‡¶æ‡¶á ‡¶è‡¶ï‡ßá \(q_ùúã\) ‡¶¶‡ßç‡¶¨‡¶æ‡¶∞‡¶æ ‡¶™‡ßç‡¶∞‡¶ï‡¶æ‡¶∂ ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡•§<br>
                                        $$ q_ùúã(s,a) = E\left(G_t | S_t = s, A_t = a\right) $$
                                        $$ q_ùúã(s,a) = E\left(\sum_{k=0}^‚àû Œ≥^kR_{t+Œ≥+1} | S_t=s, A_t = a \right) $$
                                        Agent-‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶ï‡ßã‡¶® ‡¶∏‡ßç‡¶ü‡ßá‡¶ü ‡¶≠‡¶æ‡¶≤‡ßã ‡¶∏‡ßá‡¶ü‡¶æ ‡¶¨‡ßã‡¶ù‡¶æ ‡¶Ø‡¶æ‡ßü state-value function ‡¶è‡¶∞ Expected return value ‡¶¶‡¶ø‡ßü‡ßá ‡¶Ü‡¶∞ ‡¶ï‡ßã‡¶® ‡¶∏‡ßç‡¶ü‡ßá‡¶ü‡ßá ‡¶ï‡ßã‡¶® action-‡¶ü‡¶æ ‡¶≠‡¶æ‡¶≤‡ßã ‡¶∏‡ßá‡¶ü‡¶æ ‡¶¨‡ßã‡¶ù‡¶æ ‡¶Ø‡¶æ‡ßü action-value function ‡¶è‡¶∞ Expected return value ‡¶¶‡¶ø‡ßü‡ßá‡•§ 
                                        ‡¶è‡¶á ‡¶¶‡ßÅ‡¶á Expected return value, Policy-‡¶ï‡ßá ‡¶Ö‡¶®‡ßá‡¶ï influence ‡¶ï‡¶∞‡ßá‡•§<br><br>
                                        <span class="deep-orange-text text-darken-2">‡¶è‡¶ï‡¶ü‡¶ø Optimal Policy ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßá ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ‡¶ü‡¶æ‡¶á ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ ‡¶Æ‡ßÅ‡¶≤ ‡¶â‡¶¶‡ßç‡¶¶‡ßá‡¶∂‡ßç‡¶Ø‡•§ 
                                        Optimal Policy ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßá ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø Q-learning ‡¶∏‡¶æ‡¶π‡¶æ‡¶Ø‡ßç‡¶Ø‡ßá optimal Q-function-‡¶ï‡ßá ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßá ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡•§</span><br>
                                        $$ ùúã > ùúã^/ \text {and only if } q_ùúã > {q_ùúã}^/ \text {for all } s\text{ }œµ\text{ }S \text{ and } a\text{ }œµ\text{ }A $$
                                        ‡¶è‡¶¨‡¶æ‡¶∞ ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶ú‡¶æ‡¶®‡¶¨, Q-learning ‡¶ï‡¶ø‡¶≠‡¶æ‡¶¨‡ßá ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá‡•§<br>
                                        Agent ‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡ßá Environment-‡¶ï‡ßá explore ‡¶ï‡¶∞‡¶¨‡ßá ‡¶§‡¶æ‡¶∞‡¶™‡¶∞ exploit ‡¶ï‡¶∞‡¶¨‡ßá‡•§
                                        ‡¶Æ‡¶®‡ßá ‡¶ï‡¶∞‡ßÅ‡¶®, ‡¶è‡¶ñ‡¶æ‡¶®‡ßá Agent ‡¶π‡¶≤ ‡¶ü‡¶ø‡¶ï‡¶ü‡¶ø‡¶ï‡¶ø‡•§ ‡¶∏‡ßá ‡¶∂‡ßÅ‡¶ß‡ßÅ ‡¶°‡¶æ‡¶®‡ßá, ‡¶¨‡¶æ‡¶Æ‡ßá, ‡¶â‡¶™‡¶∞‡ßá, ‡¶®‡¶ø‡¶ö‡ßá ‡¶Ø‡ßá‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡•§ 
                                        ‡¶Ø‡¶¶‡¶ø ‡¶ì‡¶ü‡¶æ ‡ßß‡¶ü‡¶æ ‡¶™‡ßã‡¶ï‡¶æ‡¶∞ ‡¶ò‡¶∞‡ßá ‡¶Ø‡¶æ‡ßü ‡¶§‡¶æ‡¶π‡¶≤‡ßá reward ‡¶™‡¶æ‡¶¨‡ßá +‡ßß‡•§ ‡¶Ø‡¶¶‡¶ø ‡¶´‡¶æ‡¶ï‡¶æ ‡¶ò‡¶∞‡ßá ‡¶Ø‡¶æ‡ßü ‡¶§‡¶æ‡¶π‡¶≤‡ßá ‡¶™‡¶æ‡¶¨‡ßá -‡ßß‡•§ 
                                        ‡¶Ø‡¶¶‡¶ø ‡¶™‡¶æ‡¶ñ‡¶ø‡¶∞ ‡¶ò‡¶∞‡ßá ‡¶Ø‡¶æ‡ßü ‡¶§‡¶æ‡¶π‡¶≤‡ßá ‡¶™‡¶æ‡¶¨‡ßá -‡ßß‡ß¶ ‡¶Æ‡¶æ‡¶®‡ßá ‡¶ñ‡ßá‡¶≤‡¶æ ‡¶∂‡ßá‡¶∑ ‡¶Ü‡¶∞ ‡¶Ø‡¶¶‡¶ø ‡ß´‡¶ü‡¶æ ‡¶™‡ßã‡¶ï‡¶æ‡¶∞ ‡¶ò‡¶∞‡ßá ‡¶Ø‡¶æ‡ßü ‡¶§‡¶æ‡¶π‡¶≤‡ßá ‡¶™‡¶æ‡¶¨‡ßá +‡ßß‡ß¶ ‡¶Æ‡¶æ‡¶®‡ßá ‡¶ñ‡ßá‡¶≤‡¶æ‡ßü ‡¶ú‡¶ø‡¶§‡ßá ‡¶ó‡ßá‡¶≤‡•§<br>
                                        Reinforcement Learning ‡¶¶‡¶ø‡ßü‡ßá ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶è‡¶Æ‡¶® ‡¶è‡¶ï‡¶ü‡¶æ Policy ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶¨ ‡¶Ø‡ßá‡¶® ‡¶§‡¶æ ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ï‡ßç‡¶∏‡¶ø‡ßü‡¶æ‡¶Æ reward gain ‡¶ï‡¶∞‡ßá ‡¶Ö‡¶∞‡ßç‡¶•‡ßç‡¶Ø‡¶æ‡ßé optimal way-‡¶§‡ßá ‡ß´‡¶ü‡¶ø ‡¶™‡ßã‡¶ï‡¶æ‡¶∞ ‡¶ò‡¶∞‡ßá ‡¶™‡ßå‡¶õ‡¶æ‡ßü‡•§
                                        ‡¶ü‡¶ø‡¶ï‡¶ü‡¶ø‡¶ï‡¶ø‡¶ü‡¶æ ‡ß´ ‡¶®‡¶Æ‡ßç‡¶¨‡¶∞ ‡¶ò‡¶∞ ‡¶•‡ßá‡¶ï‡ßá ‡¶Ø‡¶æ‡¶§‡ßç‡¶∞‡¶æ ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶ï‡¶∞‡ßá‡•§ exploration-‡¶è‡¶∞ ‡¶∏‡¶Æ‡ßü agent ‡ß´ ‡¶®‡¶Æ‡ßç‡¶¨‡¶∞ ‡¶ò‡¶∞ ‡¶•‡ßá‡¶ï‡ßá ‡¶¨‡¶ø‡¶≠‡¶ø‡¶®‡ßç‡¶® ‡¶ò‡¶∞‡ßá ‡¶ó‡¶ø‡ßü‡ßá ‡¶¶‡ßá‡¶ñ‡ßá ‡¶ï‡ßã‡¶® ‡¶ò‡¶∞‡ßá ‡¶ó‡ßá‡¶≤‡ßá ‡¶ï‡¶§ reward ‡¶™‡¶æ‡¶ì‡ßü‡¶æ ‡¶Ø‡¶æ‡¶¨‡ßá‡•§
                                        ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶è‡¶á exploration ‡¶Ü‡¶∞ exploitation- ‡¶ï‡ßá ‡¶§‡ßã balance ‡¶ï‡¶∞‡¶æ ‡¶¶‡¶∞‡¶ï‡¶æ‡¶∞‡•§<br><br>
                                        exploration-exploitation balance ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Ü‡¶õ‡ßá <span class="deep-orange-text text-darken-2">Epsilon greedy strategy</span>. 
                                        ‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‚àà ‡¶è‡¶ï‡¶ü‡¶ø ‡¶≠‡ßç‡¶Ø‡¶æ‡¶∞‡¶ø‡ßü‡ßá‡¶¨‡¶≤ ‡¶Ü‡¶õ‡ßá ‡¶Ø‡ßá‡¶ü‡¶æ‡¶∞ ‡¶≠‡ßç‡¶Ø‡¶æ‡¶≤‡ßÅ ‡ß¶ ‡¶•‡ßá‡¶ï‡ßá ‡ßß ‡¶è‡¶∞ ‡¶≠‡ßá‡¶§‡¶∞ ‡¶π‡ßü‡ßá ‡¶•‡¶æ‡¶ï‡ßá‡•§ 
                                        ‡¶≠‡ßç‡¶Ø‡¶æ‡¶≤‡ßÅ ‡ßß ‡¶Æ‡¶æ‡¶®‡ßá explore ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶∏‡¶Æ‡ßç‡¶≠‡¶æ‡¶¨‡¶®‡¶æ ‡ßß‡ß¶‡ß¶% ‡¶Ü‡¶∞ ‡¶≠‡ßç‡¶Ø‡¶æ‡¶≤‡ßÅ ‡ß¶ ‡¶Æ‡¶æ‡¶®‡ßá exploit ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶∏‡¶Æ‡ßç‡¶≠‡¶æ‡¶¨‡¶®‡¶æ ‡ßß‡ß¶‡ß¶%‡•§ 
                                        ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ implementation ‡¶è‡¶ï‡¶ü‡¶æ random threshold value, r ‡¶ß‡¶∞‡ßá ‡¶®‡¶ø‡¶¨ ‡¶Ü‡¶∞ ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶æ episode- ‡¶è ‚àà ‡¶è‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶ï‡¶Æ‡¶æ‡¶§‡ßá ‡¶•‡¶æ‡¶ï‡¶¨‡•§ 
                                        ‡¶ï‡¶Æ‡¶§‡ßá ‡¶ï‡¶Æ‡¶§‡ßá ‡¶Ø‡¶ñ‡¶® r ‡¶è‡¶∞ ‡¶®‡¶ø‡¶ö‡ßá ‡¶ö‡¶≤‡ßá ‡¶Ø‡¶æ‡¶¨‡ßá ‡¶§‡¶ñ‡¶® ‡¶•‡ßá‡¶ï‡ßá exploitation ‡¶ï‡¶∞‡¶æ ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶ï‡¶∞‡¶¨‡•§<br><br>
                                        Exploitation- ‡¶è‡¶∞ ‡¶∏‡¶Æ‡ßü ‡¶Ü‡¶Æ‡¶∞‡¶æ optimal Q-function ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶¨‡•§  <span class="deep-orange-text text-darken-2">Bellman optimality equation</span> ‡¶¶‡¶ø‡ßü‡ßá Optimal Q-function ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡•§ 
                                        ‡¶∏‡ßÇ‡¶§‡ßç‡¶∞‡¶ü‡¶ø ‡¶®‡¶ø‡¶Æ‡ßç‡¶®‡¶∞‡ßÇ‡¶™‡¶É<br>
                                        $$ q_* = E(R_{t+1} + \max\left(q_*(S_{t+1}, A_{t+1})\right)) $$
                                        ‡¶Æ‡¶®‡ßá ‡¶ï‡¶∞‡ßÅ‡¶®, ‡¶™‡ßÇ‡¶∞‡ßç‡¶¨‡¶¨‡¶∞‡ßç‡¶§‡ßÄ q-value ‡¶π‡¶≤ q ‡¶Ü‡¶∞ ‡¶Ü‡¶™‡¶°‡ßá‡¶ü‡ßá‡¶° q-value ‡¶π‡¶≤ q_*‡•§ ‡¶∏‡ßÅ‡¶§‡¶∞‡¶æ‡¶Ç,<br>
                                        $$ loss = q_* - q $$
                                        $$ loss = E(R_{t+1} + \max\left(q_*(S_{t+1}, A_{t+1})\right)) - E\left(\sum_{k=0}^‚àû Œ≥^kR_{t+Œ≥+1} | S_t=s, A_t = a \right) $$
                                        ‡¶®‡¶ø‡¶Æ‡ßç‡¶®‡¶≤‡¶ø‡¶ñ‡¶ø‡¶§ ‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏ ‡¶¶‡ßÅ‡¶á‡¶ü‡¶ø iteratively ‡¶ö‡¶æ‡¶≤‡¶æ‡¶§‡ßá ‡¶ö‡¶æ‡¶≤‡¶æ‡¶§‡ßá ‡¶è‡¶á loss ‡¶ï‡ßá ‡¶Ø‡¶§‡¶ü‡¶æ ‡¶∏‡¶Æ‡ßç‡¶≠‡¶¨ ‡ß¶ ‡¶è‡¶∞ ‡¶ï‡¶æ‡¶õ‡ßá‡¶ï‡¶æ‡¶õ‡¶ø ‡¶®‡¶ø‡ßü‡ßá ‡¶Ü‡¶∏‡¶¨‡•§<br>
                                        $$ q_{new} = q(s, a) -  Œ±*loss $$
                                        $$ q(s, a) = q_{new}(s, a) $$
                                        ‡¶è‡¶≠‡¶æ‡¶¨‡ßá‡¶á Q-learning ‡¶è‡¶∞ ‡¶∏‡¶æ‡¶π‡¶æ‡¶Ø‡ßç‡¶Ø‡ßá optimal Q-function ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡•§    
                                    </p>
                            </div>
                        </li>
                        <li>
                            <div class="collapsible-header blue lighten-5" style="font-size: 20px;">‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶Æ‡¶§‡¶æ‡¶Æ‡¶§ ‡¶ú‡¶æ‡¶®‡¶æ‡¶®.....</div>
                            <div class="collapsible-body">
                                <div class="fb-like" data-href="http://banglayml.herokuapp.com/RL_Q_learningText/" data-width="" data-layout="standard" data-action="like" data-size="large" data-show-faces="true" data-share="true"></div><br><br>
                                <div class="fb-comments" data-href="http://banglayml.herokuapp.com/RL_Q_learningText/" data-width="550" data-numposts="5"></div>
                            </div>
                        </li>
                    </ul>
                </div>
            </div>
        </div>
    </div>
{% endblock %}